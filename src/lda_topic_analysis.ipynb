{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LDA Topic Modelling from Hansard\n",
    "## Using LDA to try to extract topics from unlabelled UK parliamentary debates."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preparation\n",
    "A single-day example of the data can be found in `/data/2016-01-14.json` to make following this more clear."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import lzma\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "raw_data = None\n",
    "with lzma.open(f\"../data/2016-today.xz\", \"rb\") as f:\n",
    "    raw_data = pickle.load(f)\n",
    "\n",
    "# Check loaded successfully by printing date of first 2 days in data.\n",
    "pprint(raw_data[0][\"date\"])\n",
    "pprint(raw_data[1][\"date\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'2016-01-05'\n",
      "'2016-01-06'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we recursively loop through the \"debates\" as they have a nested structure. The problem we face here is that some things Hansard lists as \"debates\" are/have collections of debates or even just statements. So we try to filter down to the debates on the deepest level and those which have more than 1 contribution so therefore are not statements."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def recurse_debates(debates):\n",
    "    for deb in debates:  # Loop through debates\n",
    "        contributions = [\n",
    "            x for x in deb[\"items\"] if x[\"type\"] == \"contribution\"\n",
    "        ]  # Get only the items which have contributions\n",
    "        if (\n",
    "            len(contributions) > 1\n",
    "        ):  # Keep only debates that have more than one contribution.\n",
    "            yield np.array(\n",
    "                [deb[\"title\"], \" \".join([item[\"text\"] for item in deb[\"items\"]])]\n",
    "            )  # Yield debate item in format [title, joined text]\n",
    "        if (\n",
    "            \"child_debates\" in deb\n",
    "        ):  # If debate has child debates loop through those and yield items.\n",
    "            for item in recurse_debates(deb[\"child_debates\"]):\n",
    "                yield item\n",
    "\n",
    "\n",
    "def generate_debates(raw_data):\n",
    "    for day in raw_data:  # Loop through days in data\n",
    "        for item in recurse_debates(day[\"debates\"]):\n",
    "            yield item  # Yield each debate item\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list(generate_debates(raw_data)), columns=[\"title\", \"text\"])\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              title  \\\n",
       "0                              Out-of-hospital Care   \n",
       "1                                       GP Services   \n",
       "2                         Hospital Trusts: Deficits   \n",
       "3                                     Rare Diseases   \n",
       "4               Social Care Budgets: A&E Attendance   \n",
       "...                                             ...   \n",
       "13988                            Education Recovery   \n",
       "13989               Official Development Assistance   \n",
       "13990                               Points of Order   \n",
       "13991  Advanced Research and Invention  Agency Bill   \n",
       "13992     Preserving Heritage and Statues in Cities   \n",
       "\n",
       "                                                    text  \n",
       "0      1.  What progress his Department has made on i...  \n",
       "1      2.  What progress his Department is making on ...  \n",
       "2      3.  What proportion of hospital trusts are in ...  \n",
       "3      4.  How many people have diseases classified b...  \n",
       "4      5.  What assessment he has made of the effect ...  \n",
       "...                                                  ...  \n",
       "13988  With permission, Mr Deputy Speaker, I will mak...  \n",
       "13989  Application for emergency debate (Standing Ord...  \n",
       "13990  On a point of order, Mr Deputy Speaker. This i...  \n",
       "13991  [Relevant documents: Third Report of the Scien...  \n",
       "13992  Motion made, and Question proposed, That this ...  \n",
       "\n",
       "[13993 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Out-of-hospital Care</td>\n",
       "      <td>1.  What progress his Department has made on i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP Services</td>\n",
       "      <td>2.  What progress his Department is making on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hospital Trusts: Deficits</td>\n",
       "      <td>3.  What proportion of hospital trusts are in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rare Diseases</td>\n",
       "      <td>4.  How many people have diseases classified b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Social Care Budgets: A&amp;E Attendance</td>\n",
       "      <td>5.  What assessment he has made of the effect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13988</th>\n",
       "      <td>Education Recovery</td>\n",
       "      <td>With permission, Mr Deputy Speaker, I will mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13989</th>\n",
       "      <td>Official Development Assistance</td>\n",
       "      <td>Application for emergency debate (Standing Ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990</th>\n",
       "      <td>Points of Order</td>\n",
       "      <td>On a point of order, Mr Deputy Speaker. This i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13991</th>\n",
       "      <td>Advanced Research and Invention  Agency Bill</td>\n",
       "      <td>[Relevant documents: Third Report of the Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13992</th>\n",
       "      <td>Preserving Heritage and Statues in Cities</td>\n",
       "      <td>Motion made, and Question proposed, That this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13993 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "Preprocessing is at first done with [Gensim's preprocessing](https://radimrehurek.com/gensim/parsing/preprocessing.html) which does all of the usual removal of unwanted chars and stopwords, as well as tokenisation and stemming."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "# Grab just the text from the original data.\n",
    "processed_corp = preprocess_documents(df[\"text\"])\n",
    "print(processed_corp[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/luke/.pyenv/versions/3.8.6/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['progress', 'depart', 'integr', 'improv', 'care', 'provid', 'outsid', 'hospit', 'happi', 'new', 'year', 'speaker—and', 'happi', 'new', 'year', 'familiar', 'face', 'opposit', 'shadow', 'cabinet', 'govern', 'commit', 'transform', 'hospit', 'care', 'commun', 'seen', 'excel', 'progress', 'area', 'led', 'integr', 'pioneer', 'torbai', 'greenwich', 'govern', 'remain', 'fulli', 'commit', 'deliv', 'integr', 'programm', 'better', 'care', 'fund', 'vanguard', 'seventi', 'cent', 'peopl', 'prefer', 'die', 'home', 'allow', 'peopl', 'die', 'hospit', 'chang', 'netherland', 'ow', 'better', 'social', 'care', 'provid', 'outsid', 'hospit', 'messag', 'minist', 'clinic', 'commiss', 'group', 'try', 'hard', 'bring', 'integr', 'servic', 'grate', 'hon', 'friend', 'rais', 'issu', 'share', 'view', 'want', 'greater', 'choic', 'end', 'life', 'care', 'peopl', 'abl', 'care', 'die', 'place', 'choos', 'appropri', 'need', 'hospic', 'hospit', 'home', 'recent', 'choic', 'review', 'set', 'vision', 'enabl', 'greater', 'choic', 'end', 'life', 'work', 'nh', 'england', 'best', 'achiev', 'govern', 'expect', 'comment', 'soon', 'health', 'secretari', 'recent', 'receiv', 'letter', 'rang', 'social', 'care', 'organis', 'chariti', 'pan', 'spend', 'review', 'offer', 'sai', 'it“i', 'suffici', 'resolv', 'care', 'fund', 'crisis”and', 'warn', 'an“increas', 'number', 'older', 'people”without', 'suffici', 'support', '“increas', 'pressur', 'nh', '”will', 'health', 'secretari', 'final', 'admit', 'offer', 'autumn', 'statement', 'good', 'social', 'care', 'import', 'chancellor’', 'spend', 'review', 'note', 'billion', 'avail', 'social', 'care', 'precept—that', 'ad', 'council', 'tax—and', 'billion', 'avail', 'billion', 'avail', 'know', 'resourc', 'social', 'care', 'tight', 'need', 'best', 'practic', 'best', 'us', 'resourc', 'lead', 'author', 'right', 'hon', 'friend', 'consid', 'integr', 'improv', 'care', 'outsid', 'hospit', 'discuss', 'secretari', 'state', 'medic', 'people’', 'republ', 'china', 'bring', 'western', 'medicin', 'herbal', 'medicin', 'acupunctur', 'bear', 'demand', 'antibiot', 'respond', 'report', 'regul', 'herbal', 'medicin', 'practition', 'look', 'carefulli', 'dispens', 'arrang', 'small', 'scale', 'assembl', 'herbal', 'product', 'govern', 'people’', 'republ', 'china', 'interest', 'herbal', 'product', 'slightli', 'normal', 'portfolio', 'remit', 'assist', 'social', 'care', 'make', 'peopl', 'feel', 'better', 'add', 'vital', 'wellb', 'welcom', 'sure', 'local', 'area', 'taken', 'extrem', 'serious', 'thank', 'minist', 'respons', 'integr', 'improv', 'care', 'outsid', 'hospit', 'wai', 'revolutionis', 'health', 'servic', 'outlin', 'link', 'depart', 'explor', 'reduc', 'pressur', 'care', 'provis', 'outsid', 'hospit', 'facilit', 'reduc', 'pressur', 'absolut', 'number', 'pilot', 'pioneer', 'programm', 'earli', 'result', 'live', 'programm', 'penwith', 'cornwal', 'reduct', 'non', 'elect', 'admiss', 'hospit', 'reduct', 'emerg', 'admiss', 'hospit', 'hon', 'gentleman', 'right', 'better', 'social', 'care', 'better', 'integr', 'impact', 'hospit', 'admiss', 'sure', 'peopl', 'receiv', 'appropri', 'care', 'appropri', 'place', 'pleas', 'hear', 'minister’', 'refer', 'integr', 'care', 'organis', 'creat', 'constitu', 'given', 'increas', 'challeng', 'provid', 'social', 'care', 'later', 'stage', 'life', 'agre', 'model', 'need', 'look', 'support', 'abil', 'pilot', 'project', 'respond', 'differ', 'demograph', 'differ', 'area', 'enabl', 'area', 'learn', 'torbai', 'come', 'frequent', 'context', 'pleas', 'abl', 'prais', 'feet', 'like', 'point', 'involv', 'adult', 'social', 'care', 'greatli', 'affect', 'recent', 'flood', 'north', 'england', 'look', 'vulner', 'peopl', 'work', 'line', 'work', 'import', 'grate', 'rai', 'jame', 'associ', 'director', 'adult', 'social', 'servic', 'work', 'local', 'author', 'affect', 'area', 'contribut', 'look', 'vulner', 'peopl', 'period', 'report', 'appal', 'failur', 'southern', 'health', 'nh', 'foundat', 'trust', 'highlight', 'fact', 'unexpect', 'death', 'mental', 'health', 'learn', 'disabl', 'patient', 'took', 'place', 'outsid', 'hospit', 'investig', 'given', 'health', 'secretari', 'allow', 'hous', 'opportun', 'scrutinis', 'find', 'christma', 'minist', 'respond', 'todai', 'wide', 'held', 'concern', 'experi', 'nh', 'trust', 'isol', 'minist', 'agre', 'nation', 'public', 'investig', 'need', 'hon', 'ladi', 'right', 'right', 'hon', 'friend', 'health', 'secretari', 'said', 'relat', 'urgent', 'question', 'wider', 'concern', 'care', 'qualiti', 'commiss', 'look', 'pictur', 'happen', 'nation', 'death', 'investig', 'appropri', 'past', 'chang', 'govern', 'determin', 'chang', 'rang', 'thing', 'relat', 'mental', 'health', 'learn', 'disabl', 'area', 'forgotten', 'long', 'brought', 'light', 'work', 'govern']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we write out the document frequencies of each word to determine a suitable filter threshold. LDA is sensitive to very high or low occurring words so we have to filter them out to get a more respresentative picture for each document. I have chosen between 0.1% and 60%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "doc_count = 0\n",
    "frequencies = defaultdict(int)  # The doc frequencies of each token.\n",
    "for doc in processed_corp:\n",
    "    doc_count += 1\n",
    "    doc_contains = set()\n",
    "    for word in doc:\n",
    "        doc_contains.add(word)\n",
    "    for word in doc_contains:\n",
    "        frequencies[word] += 1\n",
    "\n",
    "# A sorted array of the format [(token, doc_count, frequency)]. For tokens in more than 2 docs (to reduce size for sorting).\n",
    "freq_sorted = sorted(\n",
    "    [\n",
    "        (key, val, round(val * 100.0 / doc_count, 3))\n",
    "        for (key, val) in frequencies.items()\n",
    "        if val > 2\n",
    "    ],\n",
    "    key=lambda x: x[1],\n",
    ")\n",
    "\n",
    "with open(\"../logs/sorted_frequencies.txt\", \"w\") as log_file:\n",
    "    pprint(freq_sorted, log_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "final_words = [word for (word, count, perc) in freq_sorted if 0.5 < perc < 80]\n",
    "print(final_words[:50])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['danish', 'discontinu', 'consol', 'mosul', 'coalition’', 'consul', 'overtaken', 'eco', 'wiggin', 'editori', 'juggl', 'penc', 'grit', 'collis', 'bme', 'mar', 'needi', 'subscript', 'tsunami', 'dent', 'eccentr', 'businesses’', 'cannock', 'memo', 'slowest', 'echr', 'defianc', 'patronag', 'trainer', 'mod’', 'dakin', 'chryston', 'meagr', 'discomfort', 'epidemiolog', 'arcan', 'backtrack', 'flex', 'deviz', 'bonfir', 'dwp’', 'derail', 'assassin', 'abet', 'whate', 'teen', 'scanner', 'astronom', 'coroner’', 'martyn']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is still clearly some cleaning to be done with this final list of words but we can move on for now. Next we filter all words not in the `final_words` out of the corpus. Then we can, using Gensim, create a dictionary for each word in the corpus (i.e. assign each word a unique number), and covert each document to its \"bag of words\" representation (i.e remove the ordering from the words and covert it to the numbers from the dictionary)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "filtered_corp = []\n",
    "for doc in processed_corp:\n",
    "    filtered_corp.append(np.intersect1d(doc, final_words))\n",
    "\n",
    "print(filtered_corp[:2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array(['abil', 'abl', 'absolut', 'achiev', 'ad', 'add', 'admiss', 'admit',\n",
      "       'adult', 'affect', 'agre', 'allow', 'antibiot', 'appal',\n",
      "       'appropri', 'area', 'arrang', 'assembl', 'assist', 'associ',\n",
      "       'author', 'autumn', 'avail', 'bear', 'best', 'better', 'billion',\n",
      "       'bring', 'brought', 'cabinet', 'care', 'carefulli', 'cent',\n",
      "       'challeng', 'chancellor’', 'chang', 'chariti', 'china', 'choic',\n",
      "       'choos', 'christma', 'clinic', 'come', 'comment', 'commiss',\n",
      "       'commit', 'commun', 'concern', 'consid', 'constitu', 'context',\n",
      "       'contribut', 'cornwal', 'council', 'creat', 'death', 'deliv',\n",
      "       'demand', 'demograph', 'depart', 'determin', 'die', 'differ',\n",
      "       'director', 'disabl', 'discuss', 'dispens', 'earli', 'elect',\n",
      "       'emerg', 'enabl', 'end', 'england', 'excel', 'expect', 'experi',\n",
      "       'explor', 'extrem', 'face', 'facilit', 'fact', 'failur',\n",
      "       'familiar', 'feel', 'feet', 'final', 'find', 'flood', 'forgotten',\n",
      "       'foundat', 'frequent', 'friend', 'fulli', 'fund', 'gentleman',\n",
      "       'given', 'good', 'grate', 'greater', 'greatli', 'greenwich',\n",
      "       'group', 'happen', 'happi', 'hard', 'health', 'hear', 'held',\n",
      "       'highlight', 'home', 'hospic', 'hospit', 'hous', 'impact',\n",
      "       'import', 'improv', 'increas', 'integr', 'interest', 'investig',\n",
      "       'involv', 'isol', 'issu', 'jame', 'know', 'ladi', 'later', 'lead',\n",
      "       'learn', 'led', 'letter', 'life', 'light', 'like', 'line', 'link',\n",
      "       'live', 'local', 'long', 'look', 'make', 'medic', 'medicin',\n",
      "       'mental', 'messag', 'minister’', 'model', 'nation', 'need',\n",
      "       'netherland', 'new', 'nh', 'non', 'normal', 'north', 'note',\n",
      "       'number', 'offer', 'older', 'opportun', 'opposit', 'organis',\n",
      "       'outlin', 'outsid', 'ow', 'pan', 'past', 'patient', 'peopl',\n",
      "       'people’', 'period', 'pictur', 'pilot', 'pioneer', 'place',\n",
      "       'pleas', 'point', 'portfolio', 'practic', 'practition', 'prais',\n",
      "       'prefer', 'pressur', 'product', 'programm', 'progress', 'project',\n",
      "       'provid', 'provis', 'public', 'qualiti', 'question', 'rai', 'rais',\n",
      "       'rang', 'receiv', 'recent', 'reduc', 'reduct', 'refer', 'regul',\n",
      "       'relat', 'remain', 'remit', 'report', 'republ', 'resolv',\n",
      "       'resourc', 'respond', 'respons', 'result', 'review',\n",
      "       'revolutionis', 'right', 'sai', 'said', 'scale', 'scrutinis',\n",
      "       'secretari', 'seen', 'serious', 'servic', 'set', 'seventi',\n",
      "       'shadow', 'share', 'slightli', 'small', 'social', 'soon',\n",
      "       'southern', 'spend', 'stage', 'state', 'statement', 'suffici',\n",
      "       'support', 'sure', 'taken', 'thank', 'thing', 'tight', 'todai',\n",
      "       'took', 'torbai', 'transform', 'trust', 'try', 'unexpect',\n",
      "       'urgent', 'us', 'vanguard', 'view', 'vision', 'vital', 'vulner',\n",
      "       'wai', 'want', 'warn', 'welcom', 'wellb', 'western', 'wide',\n",
      "       'wider', 'work', 'year', '”will'], dtype='<U16'), array(['abl', 'absolut', 'access', 'act', 'actual', 'admiss', 'agre',\n",
      "       'alongsid', 'answer', 'app', 'appoint', 'appropri', 'area', 'ask',\n",
      "       'assur', 'awar', 'behalf', 'believ', 'benefit', 'biggest',\n",
      "       'billion', 'book', 'chair', 'challeng', 'claim', 'clinic', 'clock',\n",
      "       'commiss', 'commit', 'complet', 'congratul', 'consequ', 'constitu',\n",
      "       'contact', 'continu', 'conveni', 'cornwal', 'cours', 'creation',\n",
      "       'current', 'dai', 'decreas', 'depart', 'difficulti', 'doctor',\n",
      "       'earli', 'easier', 'economi', 'emerg', 'enhanc', 'ensur', 'even',\n",
      "       'excel', 'experi', 'extra', 'extraordinarili', 'fact', 'friend',\n",
      "       'fund', 'gener', 'get', 'given', 'go', 'good', 'gp', 'group',\n",
      "       'guarante', 'have', 'help', 'herefordshir', 'histor', 'histori',\n",
      "       'hour', 'impact', 'includ', 'incom', 'increas', 'invest', 'job',\n",
      "       'labour’', 'ladi', 'letter', 'live', 'look', 'maintain', 'make',\n",
      "       'member', 'million', 'minister’', 'monei', 'morn', 'need', 'new',\n",
      "       'nh', 'nhs’', 'north', 'notic', 'number', 'onlin', 'parliament',\n",
      "       'past', 'patient', 'peopl', 'person', 'pervers', 'phone', 'pilot',\n",
      "       'place', 'plan', 'pleas', 'point', 'possibl', 'practic',\n",
      "       'predecessor', 'prime', 'prioriti', 'privat', 'progress', 'public',\n",
      "       'quickli', 'receiv', 'reduc', 'reduct', 'resist', 'resourc',\n",
      "       'revers', 'right', 'routin', 'run', 'rural', 'said', 'scheme',\n",
      "       'scrap', 'secretari', 'servic', 'seven', 'signific', 'slough',\n",
      "       'speaker', 'state', 'strong', 'surgeri', 'target', 'tell', 'time',\n",
      "       'todai', 'turn', 'underfund', 'wait', 'want', 'week', 'weekend',\n",
      "       'welcom', 'went', 'won', 'work', 'workforc', 'wrong', 'year'],\n",
      "      dtype='<U16')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(filtered_corp)\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in filtered_corp]\n",
    "print(list(dictionary.token2id.items())[:10])\n",
    "print(corpus_bow[10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('abil', 0), ('abl', 1), ('absolut', 2), ('achiev', 3), ('ad', 4), ('add', 5), ('admiss', 6), ('admit', 7), ('adult', 8), ('affect', 9)]\n",
      "[(1, 1), (15, 1), (22, 1), (30, 1), (41, 1), (44, 1), (47, 1), (48, 1), (62, 1), (72, 1), (91, 1), (94, 1), (97, 1), (101, 1), (103, 1), (105, 1), (122, 1), (126, 1), (137, 1), (139, 1), (143, 1), (147, 1), (148, 1), (150, 1), (151, 1), (156, 1), (168, 1), (191, 1), (194, 1), (201, 1), (204, 1), (210, 1), (213, 1), (221, 1), (227, 1), (236, 1), (239, 1), (245, 1), (255, 1), (256, 1), (265, 1), (274, 1), (275, 1), (279, 1), (301, 1), (316, 1), (333, 1), (403, 1), (430, 1), (443, 1), (456, 1), (457, 1), (465, 1), (496, 1), (504, 1), (522, 1), (542, 1), (564, 1), (566, 1), (586, 1), (805, 1), (827, 1), (828, 1), (854, 1), (855, 1), (856, 1), (857, 1), (858, 1), (859, 1), (860, 1), (861, 1), (862, 1), (863, 1), (864, 1), (865, 1), (866, 1), (867, 1), (868, 1), (869, 1), (870, 1), (871, 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('3.8.6')"
  },
  "interpreter": {
   "hash": "73dd98c30cc0f151d35df4175cb0d9ba653549e657e1b10ddd5165797a0ff531"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}